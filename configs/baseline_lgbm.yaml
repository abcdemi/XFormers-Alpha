# ===================================================================
# CONFIGURATION FOR A LIGHTGBM (TREE) BASELINE EXPERIMENT
# ===================================================================

# --- General & Data Settings ---
seed: 2025
universe: 'GOOGL'
window: 60           # Not used directly by LGBM, but good for context
horizon: 1           # Predicting t+1 forward return
freq: 'D'
# test_size is not used because we use pre-split data, but we keep it for reference

# --- Feature Engineering Settings ---
# We can use the exact same feature set as the PatchTST model for a direct comparison
features:
  price:
    - logret_1
    - logret_5
    - rsi_14
    - sma_20
    - bb_pct
  volume:
    - vol_scaled_20
  calendar:
    - dow
    - month

# --- Label Definition ---
label:
  type: 'regression'
  # The dataloader is now hardcoded to create 'logret_fwd_1',
  # so 'target' is descriptive here.
  target: 'logret_fwd_1'

# --- Model Hyperparameters ---
# This is the key section we are changing
model:
  name: 'tree'       # This tells train.py to load the TreeBaseline class
  
  # --- LightGBM-specific parameters ---
  n_estimators: 500
  learning_rate: 0.05
  num_leaves: 40
  max_depth: 8
  reg_alpha: 0.1     # L1 regularization
  reg_lambda: 0.1    # L2 regularization
  colsample_bytree: 0.8

# --- Training Parameters ---
# These are not used by the scikit-learn interface of LightGBM,
# but we keep the section for consistency.
train:
  batch_size: 0
  lr: 0.0
  epochs: 0
  device: 'cpu'

# --- Backtesting Configuration ---
backtest:
  allocator: 'directional_return' # This should match the new logic
  costs_bps: 2