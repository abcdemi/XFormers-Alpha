# ===================================================================
# CONFIGURATION FOR A TUNED LSTM BASELINE EXPERIMENT
# ===================================================================

# --- General & Data Settings ---
seed: 2025
universe: 'GOOGL'
window: 60
horizon: 1
freq: 'D'

# --- Feature Engineering Settings ---
features:
  price:
    - logret_1
    - logret_5
    - rsi_14
    - sma_20
    - bb_pct
  volume:
    - vol_scaled_20
  calendar:
    - dow
    - month

# --- Label Definition ---
label:
  type: 'regression'
  target: 'logret_fwd_1'

# --- Model Hyperparameters ---
# These are now just placeholders, the tuner will override them
model:
  name: 'lstm'
  hidden_size: 50
  depth: 2

# --- Training Parameters ---
train:
  batch_size: 32
  lr: 0.001
  epochs: 25 # We keep this fixed during the search
  device: 'auto'

# --- NEW: Hyperparameter Tuning Configuration ---
tune:
  enabled: True
  n_trials: 50  # Number of different combinations for Optuna to try
  search_space:
    hidden_size:
      type: 'int'
      low: 30
      high: 120
    depth:
      type: 'int'
      low: 1
      high: 3
    lr:
      type: 'float'
      low: 1e-4
      high: 1e-2
      log: True

# --- Backtesting Configuration ---
backtest:
  allocator: 'directional_return'
  costs_bps: 2